---
title: "Untitled"
author: "Iris Zhan"
date: "2024-11-22"
output: html_document
---


```{r}
library(glmnet)
library(caret)
library(tidyverse)
library(ggplot2)
```

```{r}
Ames<-read.csv("AmesHousing.csv")
colnames(Ames)
dim(Ames)
```


```{r}
# prepocessing
str(Ames)

missing_values <- colSums(is.na(Ames))
missing_values[missing_values > 1000]

column_tables <- lapply(Ames, table)

imbalanced_columns <- sapply(column_tables, function(tbl) {
  max(tbl) / sum(tbl) > 0.9  # Adjust the threshold as needed
})

# imbalanced columns
names(column_tables)[imbalanced_columns]

ames_data <- Ames %>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor), as.numeric)) %>%
  drop_na(SalePrice)%>%
  select(-c(Misc.Feature,Fence,Pool.QC,Fireplace.Qu, Alley))%>% #remove missing values that are larger than 1000
  select(-c(Condition.2,Roof.Matl,Street,Utilities,Land.Slope,Bsmt.Cond,Heating,Central.Air,Electrical,Low.Qual.Fin.SF,Bsmt.Half.Bath,Kitchen.AbvGr,Functional,Garage.Qual,X3Ssn.Porch)) #In my assessment, they don't offer interpretable insights, and removing unbalanced data

colnames(ames_data)
```



#Visualiztion

```{r}
## Distribution of SalePrice
ggplot(ames_data, aes(x = SalePrice)) +
  geom_histogram(binwidth = 10000, fill = "red", color = "white") +
  labs(title = "Distribution of SalePrice", x = "SalePrice", y = "Count")
```

```{r}
## SalePrice vs. Neighborhood
ggplot(ames_data,aes(x=reorder(Neighborhood,SalePrice,FUN = median),y =SalePrice)) +
  geom_boxplot(fill = "lightblue") +coord_flip() +
  labs(title = "SalePrice by Neighborhood", x = "Neighborhood", y = "SalePrice")

```

```{r}
## Year built and sale price
ggplot(ames_data, aes(x = Year.Built, y = SalePrice)) +
  geom_point(alpha = 0.5, color = "blue") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Year Built vs. SalePrice", x = "Year Built", y = "SalePrice")


```



```{r}
# Create mask for numeric and categorical features
num_features <- names(Filter(is.numeric, ames_data))[-61]

# Loop through features and generate scatterplots
for (num_features in num_features) {
  # Create a ggplot for the current feature
  p <- ggplot(ames_data, aes_string(x = num_features, y = "SalePrice")) +
    geom_point(alpha = 0.5, color = "blue") +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    labs(title = paste(num_features, "vs. SalePrice"),
         x = num_features,
         y = "SalePrice") +
    theme_minimal()
  
  # Print the plot
  print(p)
}

```
From the plot, we observed that most of the plot does not show linear relationship with SalePrice. However, there are some of features have a strong relationship with sale price such as GR.live.area, Total.Bsmt.sf, year.built, Garage.Area. It inidates that when features that have related to area it generally have a positive relationship with sale prices as it the size of a house is likely a key factor for prospective buyers. Similarly, the age of the property is strongly correlated with sales price, with newer houses typically commanding higher prices.





```{r}
## correlations between numerical variables, including the response variables Saleprice

library(corrplot)
numeric_cols <- sapply(ames_data, is.numeric)
cor_matrix <- cor(ames_data[, numeric_cols], use = "complete.obs")
corrplot(cor_matrix, method = "color", tl.col = "black", tl.cex = 0.4)


```

## Model building

```{r}
set.seed(89)
n <- nrow(Ames)
train_proportion <- 0.8
n_train <- floor(train_proportion*n)
indices <- sample(seq_len(n))

train_indices <- indices[1:n_train]
test_indices <- indices[(n_train + 1):n]

## training data
train_data <- ames_data[train_indices,]
train.x<- train_data%>%
  select(-c("PID","SalePrice"))
train.y<-log(train_data$SalePrice)

# create  binary dummy variables 
categorical.vars = colnames(train.x)[which(sapply(train.x,function(x) mode(x)=="character"))]
train.matrix = train.x[, !colnames(train.x) %in% categorical.vars, drop=FALSE]

n.train = nrow(train.matrix)
for(var in categorical.vars){
    mylevels = sort(unique(train.x[, var]))
    m = length(mylevels)
    m = ifelse(m > 2, m, 1)
    tmp.train = matrix(0, n.train, m)
    col.names = NULL
    for(j in 1:m){
      tmp.train[train.x[, var]==mylevels[j], j] = 1
      col.names = c(col.names, paste(var, '_', mylevels[j], sep=''))
      }
    colnames(tmp.train) = col.names
    train.matrix = cbind(train.matrix, tmp.train)
  }



## test dataset
test_data <- ames_data[test_indices,]
test.x<- test_data%>%
  select(-c("PID","SalePrice"))



test.y<-log(test_data$SalePrice)  
categorical.vars.test = colnames(test.x)[
  which(sapply(test.x,
                 function(x) mode(x)=="character"))]

test.matrix = test.x[, !colnames(test.x) %in% categorical.vars, 
                          drop=FALSE]
n.test = nrow(test.matrix)
for(var in categorical.vars.test){
    mylevels = sort(unique(test.x[, var]))
    m = length(mylevels)
    m = ifelse(m > 2, m, 1)
    tmp.test = matrix(0, n.test, m)
    col.names = NULL
    for(j in 1:m){
      tmp.test[test.x[, var]==mylevels[j], j] = 1
      col.names = c(col.names, paste(var, '_', mylevels[j], sep=''))
      }
    colnames(tmp.test) = col.names
    test.matrix = cbind(test.matrix, tmp.test)
  }



test.matrix%>%
  drop_na()

train.matrix%>%
  drop_na()
```


```{r}
dim(test.matrix)
dim(train.matrix)
```

## remove na from training dataset

```{r}
train.df <- as.data.frame(train.matrix)
train.df <- as.data.frame(lapply(train.df, function(col) {
  if (any(is.na(col))) {
    col[is.na(col)] <- mean(col, na.rm = TRUE)
  }
  col
}))
train.df[is.na(train.df)] <- apply(train.df, 2, function(col) mean(col, na.rm = TRUE))


```


## Build LASSO model 

```{r}
train.matrix.cleaned <- as.matrix(train.df)
set.seed(89)
## Lasso model when alpha=1
lasso_model <- cv.glmnet(
  x = train.matrix.cleaned, 
  y = train.y, 
  alpha = 1
)


```

## Check cross validation error

```{r}
plot(lasso_model) 
```
## select optimal lambda

```{r}
best_lambda<-lasso_model$lambda.min  # Optimal lambda
best_lambda
```

## refit the model

```{r}
# Refit the Lasso model with the best lambda

lasso_best <- glmnet(train.matrix.cleaned, train.y, alpha = 1, lambda = best_lambda)

```


## Model performance 

```{r}
for (i in 1:ncol(test.matrix)) {
  test.matrix[is.na(test.matrix[, i]), i] <- median(test.matrix[, i], na.rm = TRUE)
}
# Make predictions on the test dataset
predictions <- predict(lasso_model, s = best_lambda, newx = as.matrix(test.matrix))
rmse <- sqrt(mean((predictions - test.y)^2))
rmse
```

## selected features 

```{r}
# Extract non-zero coefficients from the fitted Lasso model
lasso_coefficients <- coef(lasso_best) 
non_zero_indices <- which(lasso_coefficients != 0)
selected_features <- rownames(lasso_coefficients)[non_zero_indices]
selected_features <- selected_features[-1]  # Exclude the intercept
selected_features
```

```{r}

permutation_importance <- function(train.x, train.y){
  basline_cor <- sapply(train.x, function(feature) cor(feature, train.y, use = "complete.obs"))
  importance <- numeric(ncol(train.x))
  
  for (i in seq_len(ncol(train.x))) {
    permuted_x <- train.x
    permuted_x[, i] <- sample(permuted_x[, i])
    
    permuted_cor <- cor(permuted_x[,i], train.y, use = "complete.obs")
    importance[i] <- abs(basline_cor[i] - permuted_cor)
  }
  return(data.frame(Feature = colnames(train.x), Importance = importance))
}
importance_results <- permutation_importance(train.x, train.y)
threshold <- quantile(importance_results$Importance, 0.55)
importance_results$Category <- ifelse(
  importance_results$Importance > threshold, "High Importance", "Low Importance"
)
print(importance_results)
high_importance_features <- importance_results$Feature[importance_results$Category == "High Importance"]
low_importance_features <- importance_results$Feature[importance_results$Category == "Low Importance"]
```

```{r}
# For lasso
Type_I_features <- intersect(selected_features, low_importance_features)
type_I_error <- length(Type_I_features) / length(selected_features)
print(type_I_error)

type_II_features <- setdiff(high_importance_features, selected_features)
type_II_error <- length(type_II_features) / length(high_importance_features)
print(type_II_error)
```


## Adaptive Lasso
```{r}
ridge_model <- cv.glmnet(
  x = train.matrix.cleaned, 
  y = train.y,
  alpha = 0
)

summary(ridge_model)
ridge_coefficients <- as.vector(coef(ridge_model, s = "lambda.min"))[-1]
ridge_coefficients <- as.matrix(ridge_coefficients)
g <- 1
w <- 1/(abs(ridge_coefficients))^g

print(ncol(train.matrix.cleaned))
print(length(w))
```


```{r}
adaptive_lasso <- cv.glmnet(
  x = train.matrix.cleaned,
  y = train.y,
  alpha = 1,
  penalty.factor = w
)

best_lambda_adaptive_lasso <- adaptive_lasso$lambda.min
print(best_lambda_adaptive_lasso)
```

```{r}
adaptive_lasso_best <- glmnet(train.matrix.cleaned, train.y, alpha = 1, penalty.factor = w, lambda = best_lambda_adaptive_lasso)
summary(adaptive_lasso_best)
```

```{r}
plot(adaptive_lasso)
```

## performance 
```{r}
for (i in 1:ncol(test.matrix)) {
  test.matrix[is.na(test.matrix[, i]), i] <- median(test.matrix[, i], na.rm = TRUE)
}
# Make predictions on the test dataset
adaptive_lasso_predictions <- predict(adaptive_lasso, s = best_lambda_adaptive_lasso, newx = as.matrix(test.matrix))
rmse_adaptive_lasso <- sqrt(mean((adaptive_lasso_predictions - test.y)^2))
rmse_adaptive_lasso
```

## Select features
```{r}
adaptive_lasso_coefficients <- coef(adaptive_lasso_best)
adaptive_non_zero_indices <- which(adaptive_lasso_coefficients != 0)
adaptive_selected_features <- rownames(adaptive_lasso_coefficients)[adaptive_non_zero_indices]
adaptive_selected_features <- adaptive_selected_features[-1]
adaptive_selected_features

```

```{r}
# For adptive lasso
Type_I_features_adaptive <- intersect(adaptive_selected_features, low_importance_features)
type_I_error_adaptive <- length(Type_I_features_adaptive) / length(selected_features)
print(type_I_error_adaptive)

type_II_features_adaptive <- setdiff(high_importance_features, adaptive_selected_features)
type_II_error_adaptive <- length(type_II_features_adaptive) / length(high_importance_features)
print(type_II_error_adaptive)
```

## Now compare ridge-selected features with Lasso-selected features
```{r}
lasso_vs_adaptive_lasso <- intersect(selected_features, adaptive_selected_features)
lasso_vs_adaptive_lasso
```


## elastic net
```{r}
train.df[is.na(train.df)] <- apply(train.df, 2, function(col) mean(col, na.rm = TRUE))
library(dplyr) 
control <- trainControl(method = "cv", number = 5) 
data <- as.data.frame(train.df)
tibble::as_tibble(train.df)

# Training Elastic Net Regression model 
elastic_model <- train(train.y ~ ., 
                           data = cbind(train.y, train.df), 
                           method = "glmnet", 
                           trControl = control) 

summary(elastic_model)

get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}
get_best_result(elastic_model)


```

## performance for elastic net
```{r}
for (i in 1:ncol(test.matrix)) {
  test.matrix[is.na(test.matrix[, i]), i] <- median(test.matrix[, i], na.rm = TRUE)
}
x_hat_pre <- predict(elastic_model, test.matrix) 
rmse <- sqrt(mean((x_hat_pre - test.y)^2))
rmse
```
## Select features elastic net
```{r}
elastic_coefficients<- coef(elastic_model$finalModel, elastic_model$bestTune$lambda)
non_zero_indices <- which(elastic_coefficients != 0)
selected_features <- rownames(elastic_coefficients)[non_zero_indices]
selected_features <- selected_features[-1]  # Exclude the intercept
selected_features
```

```{r}
permutation_importance <- function(train.x, train.y){
  basline_cor <- sapply(train.x, function(feature) cor(feature, train.y, use = "complete.obs"))
  importance <- numeric(ncol(train.x))
  
  for (i in seq_len(ncol(train.x))) {
    permuted_x <- train.x
    permuted_x[, i] <- sample(permuted_x[, i])
    
    permuted_cor <- cor(permuted_x[,i], train.y, use = "complete.obs")
    importance[i] <- abs(basline_cor[i] - permuted_cor)
  }
  return(data.frame(Feature = colnames(train.x), Importance = importance))
}
importance_results <- permutation_importance(train.x, train.y)
threshold <- quantile(importance_results$Importance, 0.5)
importance_results$Category <- ifelse(
  importance_results$Importance > threshold, "High Importance", "Low Importance"
)
print(importance_results)
high_importance_features <- importance_results$Feature[importance_results$Category == "High Importance"]
low_importance_features <- importance_results$Feature[importance_results$Category == "Low Importance"]
```

```{r}
Type_I_features <- intersect(selected_features, low_importance_features)
type_I_error <- length(Type_I_features) / length(selected_features)
print(type_I_error)

type_II_features <- setdiff(high_importance_features, selected_features)
type_II_error <- length(type_II_features) / length(high_importance_features)
print(type_II_error)
```


## controlling FDR

# Benjamini-Hochberg

```{r}

fdr<-0.3
k = ncol(train.matrix.cleaned)
lm.fit = lm(train.y ~ train.matrix.cleaned-1)
p.values = coef(summary(lm.fit))[, 4]
cutoff = max(c(0, which(sort(p.values) <= fdr * (1:k) / k)))
BH_selected = names(which(p.values <= fdr * cutoff / k))
for(i in 1:length(BH_selected)){
  BH_selected[i] <- substring(BH_selected[i], 7)
}
BH_selected
BH_selected <- gsub("matrix.cleaned", "", BH_selected)
BH_selected
```

```{r}
bh_subset_data <- train.matrix.cleaned[, BH_selected]
bh_model <- lm(train.y ~ ., data = data.frame(bh_subset_data))
bh_predictions <- predict(bh_model, newdata =data.frame(test.x[,BH_selected]))
# Remove observations with NA predictions
valid_idx <- !is.na(bh_predictions)
test.y.valid <- test.y[valid_idx]
bh_predictions.valid <- bh_predictions[valid_idx]
rmse <- sqrt(mean((test.y.valid - bh_predictions.valid)^2))
print(paste("Root Mean Squared Error (RMSE):", rmse))

```
```{r}
Type_I_features_bh <- intersect(BH_selected, low_importance_features)
type_I_error <- length(Type_I_features_bh) / length(BH_selected)
print(type_I_error)

type_II_features_bh <- setdiff(high_importance_features, BH_selected)
type_II_error <- length(type_II_features_bh) / length(high_importance_features)
print(type_II_error)
```
# Knockoff

```{r}
library(knockoff)
set.seed(8)

fdr <- 0.3
X <- as.matrix(train.matrix.cleaned)
y <- train.y

knockoff_result = knockoff.filter(train.matrix.cleaned, train.y, fdr = fdr)
knockoff_selected <- colnames(X)[knockoff_result$selected]
knockoff_subset_data <- train.matrix.cleaned[, knockoff_selected]
knockoff_model <- lm(train.y ~ ., data = data.frame(knockoff_subset_data))
knockoff_predictions <- predict(knockoff_model, newdata = data.frame(test.x[, knockoff_selected]))
valid_idx <- !is.na(knockoff_predictions)
test.y.valid <- test.y[valid_idx]
knockoff_predictions.valid <- knockoff_predictions[valid_idx]


rmse <- sqrt(mean((test.y.valid - knockoff_predictions.valid)^2))
print(paste("Root Mean Squared Error (RMSE):", rmse))


Type_I_features_knockoff <- intersect(knockoff_selected, low_importance_features)
type_I_error <- length(Type_I_features_knockoff) / length(knockoff_selected)
print(type_I_error)

Type_II_features_knockoff <- setdiff(high_importance_features, knockoff_selected)
type_II_error <- length(Type_II_features_knockoff) / length(high_importance_features)
print(type_II_error)

```


